import torch
import os
# ä¿®æ­£1ï¼šæ–‡ä»¶åæ˜¯ model_minimindï¼Œç±»åæ˜¯ MiniMindForCausalLM
from model.model_minimind import MiniMindForCausalLM, MiniMindConfig
from transformers import AutoTokenizer

print("æ­£åœ¨åˆå§‹åŒ–æ¨¡åž‹é…ç½®...")
# 1. åˆå§‹åŒ–ä¸€ä¸ªâ€œç©ºè„‘å­â€é…ç½®
config = MiniMindConfig()

# ä¿®æ­£2ï¼šä½ çš„é…ç½®ç±»é‡Œå« hidden_sizeï¼Œä¸å« dim
config.hidden_size = 512 
config.num_hidden_layers = 8
config.num_attention_heads = 8

# 2. å‡­ç©ºé€ ä¸€ä¸ªæ¨¡åž‹ (éšæœºåˆå§‹åŒ–)
print("æ­£åœ¨æž„å»ºæ¨¡åž‹ç»“æž„ (éšæœºå‚æ•°)...")
model = MiniMindForCausalLM(config).to("cuda")

# 3. åŠ è½½åˆ†è¯å™¨
# ä¿®æ­£3ï¼šçœ‹æˆªå›¾ï¼Œä½ çš„ tokenizer.json åœ¨ model ç›®å½•ä¸‹ï¼Œä¸æ˜¯ dataset/tokenizer_root
tokenizer_path = './model' 
print(f"æ­£åœ¨åŠ è½½åˆ†è¯å™¨: {tokenizer_path} ...")

try:
    # å°è¯•åŠ è½½é¡¹ç›®è‡ªå¸¦çš„åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True, use_fast=False)
except Exception as e:
    print(f"âš ï¸ æœ¬åœ°åŠ è½½å¤±è´¥ ({e})ï¼Œå°è¯•ä½¿ç”¨é€šç”¨ GPT2 åˆ†è¯å™¨ä½œä¸ºæ›¿èº«...")
    from transformers import GPT2Tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token

# 4. è®©è¿™ä¸ªç©ºè„‘å­è¯´è¯
question = "ä½ å¥½ï¼Œä½ æ˜¯è°ï¼Ÿ"
print(f"\nðŸ§ è¾“å…¥çš„é—®é¢˜: {question}")

# æŠŠé—®é¢˜å˜æˆæ•°å­—
# ç¡®ä¿ pad_token_id å­˜åœ¨ï¼Œé˜²æ­¢æŠ¥é”™
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

inputs = tokenizer(question, return_tensors='pt').to("cuda")

# è®©æ¨¡åž‹ä¹±çŒœåŽé¢çš„å­—
with torch.no_grad():
    outputs = model.generate(
        inputs.input_ids, 
        max_new_tokens=20, 
        temperature=1.0,
        do_sample=True,
        top_k=5,
        pad_token_id=tokenizer.pad_token_id
    )

# æŠŠæ•°å­—å˜å›žå­—
answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"ðŸ¤ª æ²¡è®­ç»ƒçš„æ¨¡åž‹çš„å›žç­” (åº”è¯¥æ˜¯ä¹±ç ):\n >> {answer}")